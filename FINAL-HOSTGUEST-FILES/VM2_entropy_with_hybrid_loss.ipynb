{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c3ec39",
   "metadata": {},
   "source": [
    "# Initialize all imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e70939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 21:29:13.555919: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-30 21:29:13.622861: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (/home/lthoma21/.local/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/lthoma21/.local/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import conda_installer\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from deepchem.feat.graph_features import atom_features as get_atom_features\n",
    "import rdkit\n",
    "import pickle\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import importlib\n",
    "import keras.backend as K\n",
    "# import tensorflow_addons as tfa\n",
    "from tensorflow.keras import regularizers, constraints, callbacks\n",
    "\n",
    "import sys\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8e0a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDBs = pickle.load(open('Datasets/PDBs_RDKit.pkl', 'rb'))\n",
    "df = pd.read_csv('Datasets/T_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "530b15a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>TS_comp</th>\n",
       "      <th>TS_host</th>\n",
       "      <th>TS_ligand</th>\n",
       "      <th>exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>gilson_cb7--guest1</td>\n",
       "      <td>219.726160</td>\n",
       "      <td>170.780918</td>\n",
       "      <td>32.569296</td>\n",
       "      <td>-0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>gilson_cb7--guest2</td>\n",
       "      <td>243.211795</td>\n",
       "      <td>170.780918</td>\n",
       "      <td>54.026506</td>\n",
       "      <td>-0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>gilson_cb7--guest3</td>\n",
       "      <td>228.890715</td>\n",
       "      <td>170.780918</td>\n",
       "      <td>41.655571</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>gilson_cb7--guest4</td>\n",
       "      <td>203.927320</td>\n",
       "      <td>170.780918</td>\n",
       "      <td>19.589363</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>gilson_cb7--guest5</td>\n",
       "      <td>266.219403</td>\n",
       "      <td>170.780918</td>\n",
       "      <td>80.367673</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  Id     TS_comp     TS_host  TS_ligand  exp\n",
       "0           0  gilson_cb7--guest1  219.726160  170.780918  32.569296 -0.9\n",
       "1           1  gilson_cb7--guest2  243.211795  170.780918  54.026506 -0.4\n",
       "2           2  gilson_cb7--guest3  228.890715  170.780918  41.655571 -0.1\n",
       "3           3  gilson_cb7--guest4  203.927320  170.780918  19.589363  0.6\n",
       "4           4  gilson_cb7--guest5  266.219403  170.780918  80.367673  0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf9db70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(PDBs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfad044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "for pdb in list(PDBs.keys()):\n",
    "    info.append(df[df['Id'] == pdb][['TS_comp', 'TS_host', 'TS_ligand']].to_numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e11687d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.dcFeaturizer import atom_features as get_atom_features\n",
    "def featurize(molecule, info):\n",
    "    \n",
    "    atom_features = []\n",
    "    for atom in molecule.GetAtoms():\n",
    "        new_feature = get_atom_features(atom).tolist()\n",
    "        position = molecule.GetConformer().GetAtomPosition(atom.GetIdx())\n",
    "        new_feature += [atom.GetMass(), atom.GetAtomicNum(),atom.GetFormalCharge()]\n",
    "        new_feature += [position.x, position.y, position.z]\n",
    "        for neighbor in atom.GetNeighbors()[:2]:\n",
    "            neighbor_idx = neighbor.GetIdx()\n",
    "            new_feature += [neighbor_idx]\n",
    "        for i in range(2 - len(atom.GetNeighbors())):\n",
    "            new_feature += [-1]\n",
    "\n",
    "        atom_features.append(np.concatenate([new_feature, info], 0))\n",
    "    return np.array(atom_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9014c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i, pdb in enumerate(list(PDBs.keys())):\n",
    "    X.append(featurize(PDBs[pdb], info[i]))\n",
    "    y.append(df[df['Id'] == pdb]['exp'].to_numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98487fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "# Randomly shuffles the data before splitting, ensuring that the training and testing sets are representative of the overall dataset.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=82)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a70410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 21, 82, 21)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db67487f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 41)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276fbde6",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80798974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.layers_update_mobley' from '/home/lthoma21/BFE-Loss-Function/FINAL-HOSTGUEST-FILES/models/layers_update_mobley.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import models.layers_update_mobley as layers\n",
    "from models.dcFeaturizer import atom_features as get_atom_features\n",
    "importlib.reload(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83b1ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGGCNModel(tf.keras.Model):\n",
    "    def __init__(self, num_atom_features=36, r_out_channel=20, c_out_channel=128, l2=1e-4, dropout_rate=0.2, maxnorm=3.0):\n",
    "        super().__init__()\n",
    "        self.ruleGraphConvLayer = layers.RuleGraphConvLayer(r_out_channel, num_atom_features, 0)\n",
    "        self.ruleGraphConvLayer.combination_rules = []\n",
    "        self.conv = layers.ConvLayer(c_out_channel, r_out_channel)\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation='relu', name='dense1', kernel_regularizer=regularizers.l2(l2), bias_regularizer=regularizers.l2(l2), kernel_constraint=constraints.MaxNorm(maxnorm))\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dense5 = tf.keras.layers.Dense(16, activation='relu', name='dense2', kernel_regularizer=regularizers.l2(l2), bias_regularizer=regularizers.l2(l2), kernel_constraint=constraints.MaxNorm(maxnorm))\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dense6 = tf.keras.layers.Dense(1, name='dense6', kernel_regularizer=regularizers.l2(l2), bias_regularizer=regularizers.l2(l2), kernel_constraint=constraints.MaxNorm(maxnorm))\n",
    "        self.dense7 = tf.keras.layers.Dense(1, name='dense7',\n",
    "                                             kernel_initializer=tf.keras.initializers.Constant([-.3, -1, 1, 1]),\n",
    "                                             bias_initializer=tf.keras.initializers.Zeros(),\n",
    "                                             kernel_regularizer=regularizers.l2(l2), \n",
    "                                             bias_regularizer=regularizers.l2(l2), \n",
    "                                             kernel_constraint=constraints.MaxNorm(maxnorm))\n",
    "        self.all_layer_1_weights = []\n",
    "\n",
    "    def addRule(self, rule, start_index, end_index=None):\n",
    "        self.ruleGraphConvLayer.addRule(rule, start_index, end_index)\n",
    "        \n",
    "    def set_input_shapes(self, i_s):\n",
    "        self.i_s = i_s\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        print(\"Inside call\")\n",
    "        physics_info = inputs[:, 0, 38:] \n",
    "        x_a = []\n",
    "        for i in range(len(self.i_s)):\n",
    "            x_a.append(inputs[i][:self.i_s[i], :38])\n",
    "        x = self.ruleGraphConvLayer(x_a)\n",
    "        self.all_layer_1_weights.append(self.ruleGraphConvLayer.w_s)\n",
    "        x = self.conv(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense5(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        model_var = self.dense6(x)\n",
    "        merged = tf.concat([model_var, physics_info], axis=1)\n",
    "        out = self.dense7(merged)\n",
    "        return tf.concat([out, physics_info], axis=1)\n",
    "    \n",
    "class LossComponentsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,model_instance):\n",
    "        super().__init__()\n",
    "        self.empirical_losses = []\n",
    "        self.physical_losses = []\n",
    "        self.total_losses = []\n",
    "        self.learning_rates = []\n",
    "        self.model = model_instance\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        # Store the total loss\n",
    "        self.total_losses.append(logs.get('loss'))\n",
    "        lr = self.model.optimizer.learning_rate\n",
    "        if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            lr = lr(self.model.optimizer.iterations)  # Call the schedule\n",
    "        else:\n",
    "            lr = lr  \n",
    "\n",
    "        self.learning_rates.append(float(tf.keras.backend.get_value(lr)))\n",
    "\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred[0] - y_true))) + K.abs(1 / K.mean(.2 + y_pred[1]))\n",
    "\n",
    "\n",
    "def pure_rmse(y_true, y_pred):\n",
    "    y_true_flat = tf.reshape(y_true, [-1])\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true_flat)))\n",
    "\n",
    "def physical_consistency_loss(y_true,y_pred,physics_info):\n",
    "    dS_pred = y_pred\n",
    "    y_true = tf.reshape(y_true, (-1, 1))\n",
    "\n",
    "    # Physical Inconsistency loss\n",
    "    # Extract the components from physics_info\n",
    "    host = tf.gather(physics_info, [1], axis=1)  # Host energy terms\n",
    "    guest = tf.gather(physics_info, [2], axis=1)  # Guest energy terms\n",
    "    complex_ = tf.gather(physics_info, [0], axis=1)  # Complex energy terms\n",
    "    \n",
    "#     tf.print(\"Host:\", host)\n",
    "#     tf.print(\"Guest:\", guest)\n",
    "#     tf.print(\"Complex:\", complex_)\n",
    "\n",
    "    # Calculate ΔG based on physics: ΔS = ΔScomplex - (ΔShost + ΔSguest)\n",
    "    dS_physics = -tf.reduce_sum(complex_, axis=1, keepdims=True) + tf.reduce_sum(host, axis=1, keepdims=True) + tf.reduce_sum(guest, axis=1, keepdims=True)\n",
    "    phy_loss = K.sqrt(K.mean(K.square(dS_pred - dS_physics)))\n",
    "#     tf.print(\"DS_pred\",dS_pred)\n",
    "#     tf.print(\"DS_physics\",dS_physics)\n",
    "\n",
    "    return phy_loss\n",
    "\n",
    "\n",
    "\n",
    "def combined_loss(physics_hyperparam=0.0003):\n",
    "    def loss_function(y_true, y_pred):\n",
    "        # Extract prediction and physics info\n",
    "        prediction = y_pred[:, 0]\n",
    "        physics_info = y_pred[:, 1:4]  # Assuming 15 physical features\n",
    "        \n",
    "        # Calculate individual loss components\n",
    "        empirical_loss = pure_rmse(y_true, prediction)\n",
    "        physics_loss = physical_consistency_loss(y_true, prediction, physics_info)\n",
    "        \n",
    "        # Combine losses with weights\n",
    "        total_loss = empirical_loss + (physics_hyperparam * physics_loss)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    return loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e652993d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Hyperparameter combinations ------------\n",
      "Epoch : 500;  physics_weight: 0.005;\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 21:29:23.029204: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-04-30 21:29:23.029239: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (curie.cluster): /proc/driver/nvidia/version does not exist\n",
      "2025-04-30 21:29:23.029982: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside call\n",
      "WARNING:tensorflow:From /opt/calstatela/mambaforge/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "Inside call\n",
      "1/1 [==============================] - 78s 78s/step - loss: 12.9264\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.8080\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 1s 801ms/step - loss: 8.7518\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.8096\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.1112\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9768\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8597\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.5149\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.2089\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.6235\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.7209\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.5486\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.1799\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.7028\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.2258\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8768\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7605\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8741\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.1003\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.3074\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.4173\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.4067\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.2910\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.1110\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9239\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7896\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7466\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7912\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8806\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9621\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9992\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9803\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9169\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8357\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7680\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7369\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7465\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7817\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8184\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8371\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8297\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8014\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7658\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7382\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7283\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7361\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7528\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7669\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7704\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7620\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7464\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7316\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7238\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7248\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7316\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7386\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7412\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7380\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7309\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7238\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7199\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7202\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7232\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7262\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7270\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7250\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7214\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7181\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7166\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7170\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7184\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7194\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7191\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7176\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7156\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7142\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7138\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7141\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7146\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7146\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7138\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7127\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7117\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7111\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7110\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7111\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7109\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7104\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7097\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7090\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7084\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7082\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7080\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7077\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7072\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7067\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7061\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7056\n",
      "Epoch 99/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 3.7052\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7049\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7046\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7041\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7036\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7031\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7026\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7022\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7018\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7014\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7009\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7004\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6999\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6995\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6990\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6986\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6981\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6976\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6971\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6966\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6961\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6956\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6952\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6946\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6941\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6936\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6931\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6926\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6921\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6916\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6910\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6905\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6899\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6894\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6889\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6883\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6878\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6872\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6866\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6861\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6855\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6849\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6843\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6837\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6832\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6826\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6820\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6813\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6807\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6801\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6795\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6789\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6782\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6776\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6769\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6763\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6756\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6750\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6743\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6736\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6729\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6722\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6716\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6708\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6701\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6694\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6687\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6680\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6672\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6665\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6658\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6650\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6642\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6635\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6627\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6619\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6611\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6603\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6595\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6587\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6579\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6571\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6562\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6554\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6545\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6537\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6528\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6519\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6510\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6501\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6492\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6483\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6474\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6465\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6455\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6446\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6436\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6427\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6417\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6407\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6397\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6387\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6377\n",
      "Epoch 202/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 3.6367\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6357\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6346\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6336\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6325\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6315\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6304\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6293\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6282\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6271\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6260\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6249\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6237\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6226\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6214\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6203\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6191\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6179\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6167\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6155\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6143\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6130\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6118\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6105\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6093\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6080\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6067\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6054\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6041\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6028\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6015\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6001\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5988\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5974\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5960\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5946\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5933\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5918\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5904\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5890\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5876\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5861\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5846\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5832\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5817\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5802\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5787\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5772\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5756\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5741\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5725\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5710\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5694\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5678\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5662\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5646\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5630\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5613\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5597\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5580\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5564\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5547\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5530\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5513\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5496\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5479\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5461\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5444\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5426\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5409\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5391\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5373\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5355\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5337\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5319\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5300\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5282\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5263\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5245\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5226\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5207\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5188\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5169\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5150\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5131\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5111\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5092\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5072\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5053\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5033\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5013\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4993\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4973\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4953\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4932\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4912\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4892\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4871\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4850\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4830\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4809\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4788\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4767\n",
      "Epoch 305/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 3.4746\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4724\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4703\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4682\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4660\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4639\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4617\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4595\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4573\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4551\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4529\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4507\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4485\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4463\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4440\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4418\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4395\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4373\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4350\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4328\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4305\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4282\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4259\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4236\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4213\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4190\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4166\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4143\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4120\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4096\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4073\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4049\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4025\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4002\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3978\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3954\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3930\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3906\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3882\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3858\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3834\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3810\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3785\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3761\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3737\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3712\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3688\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3663\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3639\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3552\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3466\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3381\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3295\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3211\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3126\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3042\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2958\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2874\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2791\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2708\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2625\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2543\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2461\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2380\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2299\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2219\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2138\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2058\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1979\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1900\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1822\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1743\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1666\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1588\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1511\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1435\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1359\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1283\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1208\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1133\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1059\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0985\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0911\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0838\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0765\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0693\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0621\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0550\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0479\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0408\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0338\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0268\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0199\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0130\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.0062\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9994\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9926\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9859\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9792\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9726\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9660\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9594\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9529\n",
      "Epoch 408/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 2.9465\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9400\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9337\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9273\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9210\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9148\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9086\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.9024\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8963\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8902\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8841\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8781\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8721\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8662\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8603\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8545\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8487\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8429\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8372\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8315\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8259\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8203\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8147\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8092\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8037\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7983\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7929\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7875\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7822\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7769\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7716\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7664\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7613\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7561\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7510\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7460\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7410\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7360\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7311\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7262\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7213\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7165\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7117\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7069\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7022\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6975\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6929\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6883\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6837\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6792\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6747\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6702\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6658\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6614\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6570\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6527\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6484\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6441\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6399\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6357\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6316\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6275\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6234\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6193\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6153\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6113\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6074\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6035\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5996\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5957\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5919\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5881\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5844\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5807\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5770\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5733\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5697\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5661\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5625\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5590\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5555\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5520\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5485\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5451\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5417\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5384\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5351\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5318\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5285\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5253\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5220\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5189\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5157\n",
      "Inside call\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "Inside call\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.9001\n",
      "The mean absolute difference between y_tru & y_pred is : 1.065615548996698\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "physics_hyperparam = [0.005]\n",
    "epochs = [500]\n",
    "lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=0.005,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9,\n",
    "        staircase=True\n",
    "    )\n",
    "\n",
    "y_differences = []\n",
    "total_losses = []\n",
    "empirical_losses = []\n",
    "physics_losses = []\n",
    "all_results=[]\n",
    "    \n",
    "for epoch in epochs:\n",
    "        for physics_weight in physics_hyperparam:\n",
    "            print(\"---------- Hyperparameter combinations ------------\")\n",
    "            print(\"Epoch : {};  physics_weight: {};\".format(str(epoch),  str(physics_weight)))\n",
    "\n",
    "            m = PGGCNModel()\n",
    "            m.addRule(\"sum\", 0, 32)\n",
    "            m.addRule(\"multiply\", 32, 33)\n",
    "            m.addRule(\"distance\", 33, 36)\n",
    "\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "            m.compile(loss=combined_loss(physics_weight), optimizer=opt)\n",
    "\n",
    "            input_shapes = []\n",
    "            for i in range(len(X_train)):\n",
    "                input_shapes.append(np.array(X_train[i]).shape[0])\n",
    "            m.set_input_shapes(input_shapes)\n",
    "            for i in range(len(X_train)):\n",
    "                if X_train[i].shape[0] < 2000:\n",
    "                    new_list = np.zeros([2000 - X_train[i].shape[0], 41])\n",
    "                    X_train[i] = np.concatenate([X_train[i], new_list], 0)\n",
    "            X_train = np.array(X_train)\n",
    "            y_train = np.array(y_train)\n",
    "            \n",
    "\n",
    "            loss_tracker = LossComponentsCallback(m)\n",
    "\n",
    "            # Add early stopping\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='loss',           \n",
    "                patience=10,              \n",
    "                restore_best_weights=True, \n",
    "                min_delta=0.001,          \n",
    "                verbose=1                 \n",
    "            )\n",
    "\n",
    "            hist = m.fit(X_train, y_train, epochs = epoch, batch_size=len(X_train),callbacks=[early_stopping,loss_tracker])\n",
    "\n",
    "\n",
    "            input_shapes = []\n",
    "            for i in range(len(X_test)):\n",
    "                input_shapes.append(np.array(X_test[i]).shape[0])\n",
    "            m.set_input_shapes(input_shapes)\n",
    "\n",
    "            for i in range(len(X_test)):\n",
    "                if X_test[i].shape[0] < 2000:\n",
    "                    new_list = np.zeros([2000 - X_test[i].shape[0], 41])\n",
    "                    X_test[i] = np.concatenate([X_test[i], new_list], 0)\n",
    "            X_test = np.array(X_test)\n",
    "            x_c = copy.deepcopy(X_test)\n",
    "            y_test = np.array(y_test)\n",
    "            y_pred_test = m.predict(X_test)\n",
    "            y_pred_test = np.array(y_pred_test[:,0])\n",
    "\n",
    "            y_difference = np.mean(np.abs(np.abs(y_test) - np.abs(y_pred_test)))\n",
    "            eval = m.evaluate(X_test, y_test)\n",
    "            print(\"The mean absolute difference between y_tru & y_pred is : {}\" .format(str(y_difference)))\n",
    "\n",
    "            final_train_loss = loss_tracker.total_losses[-1] if loss_tracker.total_losses else None\n",
    "            \n",
    "        # Plot all loss components over epochs\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        epoch_length = range(1, len(loss_tracker.total_losses) + 1)\n",
    "\n",
    "        # Total loss\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(epoch_length, loss_tracker.total_losses, 'b-', label='Total Loss')\n",
    "        plt.title('Total Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "            \n",
    "\n",
    "#     # Learning rate\n",
    "#     plt.subplot(2, 2, 2)\n",
    "#     plt.plot(epochs, loss_tracker.learning_rates, 'g-', label='Learning Rate')\n",
    "#     plt.title('Learning Rate')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Learning Rate')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('loss_components.png')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4336052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
