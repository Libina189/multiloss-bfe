{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d0f9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 11:40:06.042950: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-29 11:40:06.115452: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (/home/lthoma21/.local/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/lthoma21/.local/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import conda_installer\n",
    "from rdkit import Chem\n",
    "from deepchem.feat.graph_features import atom_features as get_atom_features\n",
    "import rdkit\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5682e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDBs = pickle.load(open('Datasets/PDBs_TDS_NMA.pkl', 'rb'))\n",
    "df = pd.read_csv('Datasets/NMAEntropy_65.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6c859d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(PDBs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2989c7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3086b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>complex_name</th>\n",
       "      <th>TS_comp</th>\n",
       "      <th>TS_host</th>\n",
       "      <th>TS_ligand</th>\n",
       "      <th>DELTA_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61</td>\n",
       "      <td>cb7-18</td>\n",
       "      <td>108.6247</td>\n",
       "      <td>94.1358</td>\n",
       "      <td>28.8836</td>\n",
       "      <td>-13.4130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62</td>\n",
       "      <td>cb7-20</td>\n",
       "      <td>110.0839</td>\n",
       "      <td>94.1358</td>\n",
       "      <td>29.3665</td>\n",
       "      <td>-12.2029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>63</td>\n",
       "      <td>cb7-22</td>\n",
       "      <td>108.6411</td>\n",
       "      <td>94.1358</td>\n",
       "      <td>31.1067</td>\n",
       "      <td>-13.4184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>cb7-23</td>\n",
       "      <td>105.1555</td>\n",
       "      <td>94.1358</td>\n",
       "      <td>25.6966</td>\n",
       "      <td>-19.5447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>cb7-24</td>\n",
       "      <td>110.1865</td>\n",
       "      <td>94.1358</td>\n",
       "      <td>31.4818</td>\n",
       "      <td>-16.7849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0 complex_name   TS_comp  TS_host  TS_ligand  DELTA_S\n",
       "60          61       cb7-18  108.6247  94.1358    28.8836 -13.4130\n",
       "61          62       cb7-20  110.0839  94.1358    29.3665 -12.2029\n",
       "62          63       cb7-22  108.6411  94.1358    31.1067 -13.4184\n",
       "63          64       cb7-23  105.1555  94.1358    25.6966 -19.5447\n",
       "64          65       cb7-24  110.1865  94.1358    31.4818 -16.7849"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a80c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "for pdb in list(PDBs.keys()):\n",
    "    info.append(df[df['complex_name'] == pdb][['TS_comp', 'TS_host', 'TS_ligand']].to_numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a53565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.dcFeaturizer import atom_features as get_atom_features\n",
    "def featurize(molecule, info):\n",
    "    \n",
    "    atom_features = []\n",
    "    for atom in molecule.GetAtoms():\n",
    "        new_feature = get_atom_features(atom).tolist()\n",
    "        position = molecule.GetConformer().GetAtomPosition(atom.GetIdx())\n",
    "        new_feature += [atom.GetMass(), atom.GetAtomicNum(),atom.GetFormalCharge()]\n",
    "        new_feature += [position.x, position.y, position.z]\n",
    "        for neighbor in atom.GetNeighbors()[:2]:\n",
    "            neighbor_idx = neighbor.GetIdx()\n",
    "            new_feature += [neighbor_idx]\n",
    "        for i in range(2 - len(atom.GetNeighbors())):\n",
    "            new_feature += [-1]\n",
    "        atom_features.append(np.concatenate([new_feature, info], 0))\n",
    "    # print(len(atom_features[0]))\n",
    "    return np.array(atom_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dec3e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i, pdb in enumerate(list(PDBs.keys())):\n",
    "    X.append(featurize(PDBs[pdb], info[i]))\n",
    "    y.append(df[df['complex_name'] == pdb]['DELTA_S'].to_numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8716655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((71, 41), -1.2985)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape, y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "688eb041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "137ac02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d515be96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 13, 52, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29a963d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train[0],X_train[0],info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc510e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 11:40:22.712800: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-04-29 11:40:22.712835: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (curie.cluster): /proc/driver/nvidia/version does not exist\n",
      "2025-04-29 11:40:22.713281: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/calstatela/mambaforge/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "1/1 [==============================] - 48s 48s/step - loss: 1.3839\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 1.8836\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 1.4287\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 1s 689ms/step - loss: 1.4951\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 1s 676ms/step - loss: 1.6468\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 1s 676ms/step - loss: 1.5399\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 1.3900\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 1s 632ms/step - loss: 1.4051\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 1.5033\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 1s 670ms/step - loss: 1.5115\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 1.4346\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 1s 665ms/step - loss: 1.3745\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 1s 676ms/step - loss: 1.3981\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 1.4495\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 1s 671ms/step - loss: 1.4523\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 1s 680ms/step - loss: 1.4084\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 1s 675ms/step - loss: 1.3737\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 1s 676ms/step - loss: 1.3859\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 1s 676ms/step - loss: 1.4167\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 1s 653ms/step - loss: 1.4205\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 1s 659ms/step - loss: 1.3948\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 1s 663ms/step - loss: 1.3732\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 1s 653ms/step - loss: 1.3798\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 1s 650ms/step - loss: 1.3987\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 1.4012\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 1s 676ms/step - loss: 1.3851\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 1s 679ms/step - loss: 1.3723\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 1s 684ms/step - loss: 1.3774\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 1s 692ms/step - loss: 1.3889\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 1s 676ms/step - loss: 1.3889\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 1s 689ms/step - loss: 1.3781\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 1s 687ms/step - loss: 1.3716\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 1s 669ms/step - loss: 1.3766\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 1s 677ms/step - loss: 1.3831\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 1s 663ms/step - loss: 1.3808\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 1s 682ms/step - loss: 1.3736\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 1s 653ms/step - loss: 1.3717\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.3762\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 1s 673ms/step - loss: 1.3788\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 1s 663ms/step - loss: 1.3753\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 1s 663ms/step - loss: 1.3714\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 1s 681ms/step - loss: 1.3724\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 1.3754\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 1.3751\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 1.3720\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 1.3710\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 1s 672ms/step - loss: 1.3729\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 1.3738\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 1s 663ms/step - loss: 1.3721\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 1s 649ms/step - loss: 1.3706\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 1s 656ms/step - loss: 1.3714\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 1s 682ms/step - loss: 1.3725\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 1s 651ms/step - loss: 1.3718\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 1.3705\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 1s 623ms/step - loss: 1.3706\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 1.3715\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 1s 646ms/step - loss: 1.3713\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 1.3703\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 1.3702\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 1s 638ms/step - loss: 1.3707\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 1s 654ms/step - loss: 1.3707\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 1s 641ms/step - loss: 1.3700\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 1s 649ms/step - loss: 1.3698\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 1s 672ms/step - loss: 1.3702\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 1s 682ms/step - loss: 1.3702\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 1s 679ms/step - loss: 1.3697\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 1s 642ms/step - loss: 1.3695\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 1s 633ms/step - loss: 1.3698\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 1s 651ms/step - loss: 1.3697\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 1s 675ms/step - loss: 1.3694\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 1s 663ms/step - loss: 1.3693\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 1s 646ms/step - loss: 1.3694\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.3693\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 1s 672ms/step - loss: 1.3691\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 1s 653ms/step - loss: 1.3690\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 1s 662ms/step - loss: 1.3690\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 1s 638ms/step - loss: 1.3689\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.3687\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 1.3687\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 1s 650ms/step - loss: 1.3687\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 1s 684ms/step - loss: 1.3686\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 1.3684\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 1s 659ms/step - loss: 1.3683\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 1s 700ms/step - loss: 1.3683\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 1s 654ms/step - loss: 1.3682\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 1.3681\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 1.3680\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 1.3679\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 1s 684ms/step - loss: 1.3678\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 1s 662ms/step - loss: 1.3677\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 1s 635ms/step - loss: 1.3677\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 1s 669ms/step - loss: 1.3676\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 1.3674\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 1s 653ms/step - loss: 1.3674\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 1s 654ms/step - loss: 1.3673\n",
      "Epoch 96/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 619ms/step - loss: 1.3672\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 1s 662ms/step - loss: 1.3670\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.3670\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 1s 643ms/step - loss: 1.3669\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 1s 653ms/step - loss: 1.3667\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 1s 651ms/step - loss: 1.3666\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 1s 674ms/step - loss: 1.3665\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 1.3664\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 1s 665ms/step - loss: 1.3663\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 1s 663ms/step - loss: 1.3662\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 1s 642ms/step - loss: 1.3661\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 1s 636ms/step - loss: 1.3660\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 1s 674ms/step - loss: 1.3658\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 1.3657\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 1.3656\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 1s 643ms/step - loss: 1.3655\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 1s 640ms/step - loss: 1.3654\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 1s 644ms/step - loss: 1.3652\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 1s 646ms/step - loss: 1.3651\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 1s 665ms/step - loss: 1.3650\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 1s 672ms/step - loss: 1.3648\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 1s 656ms/step - loss: 1.3647\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 1s 679ms/step - loss: 1.3646\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 1s 686ms/step - loss: 1.3644\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 1s 701ms/step - loss: 1.3643\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 1s 688ms/step - loss: 1.3641\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 1s 709ms/step - loss: 1.3640\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 1s 659ms/step - loss: 1.3638\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 1s 669ms/step - loss: 1.3637\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 1s 680ms/step - loss: 1.3635\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 1s 643ms/step - loss: 1.3634\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 1s 663ms/step - loss: 1.3632\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 1s 656ms/step - loss: 1.3631\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 1s 643ms/step - loss: 1.3629\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 1.3627\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 1s 644ms/step - loss: 1.3626\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 1.3624\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 1s 643ms/step - loss: 1.3622\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 1s 656ms/step - loss: 1.3620\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 1s 657ms/step - loss: 1.3619\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 1s 634ms/step - loss: 1.3617\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 1.3615\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 1s 656ms/step - loss: 1.3613\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 1s 629ms/step - loss: 1.3611\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 1.3609\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 1s 654ms/step - loss: 1.3607\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 1.3605\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 1s 639ms/step - loss: 1.3603\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 1s 649ms/step - loss: 1.3601\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 1s 654ms/step - loss: 1.3599\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 1s 656ms/step - loss: 1.3597\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.3595\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 1.3593\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 1s 665ms/step - loss: 1.3591\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 1s 675ms/step - loss: 1.3589\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 1s 653ms/step - loss: 1.3586\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 1s 625ms/step - loss: 1.3584\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 1s 650ms/step - loss: 1.3582\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 1s 651ms/step - loss: 1.3579\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 1s 646ms/step - loss: 1.3577\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 1s 657ms/step - loss: 1.3575\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 1s 695ms/step - loss: 1.3572\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 1s 677ms/step - loss: 1.3570\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.3567\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 1s 679ms/step - loss: 1.3565\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 1.3562\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 1s 659ms/step - loss: 1.3560\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 1.3557\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 1.3555\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 1s 677ms/step - loss: 1.3552\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 1s 688ms/step - loss: 1.3549\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 1s 642ms/step - loss: 1.3547\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 1s 639ms/step - loss: 1.3544\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 1s 644ms/step - loss: 1.3541\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 1s 665ms/step - loss: 1.3539\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 1.3536\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 1.3533\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 1s 652ms/step - loss: 1.3530\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 1s 644ms/step - loss: 1.3527\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 1s 657ms/step - loss: 1.3524\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 1.3521\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 1s 651ms/step - loss: 1.3519\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 1.3516\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 1s 682ms/step - loss: 1.3513\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 1.3510\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 1s 644ms/step - loss: 1.3506\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 1s 674ms/step - loss: 1.3503\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 1s 644ms/step - loss: 1.3500\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 1.3497\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 1.3494\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 1s 656ms/step - loss: 1.3491\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 1s 675ms/step - loss: 1.3488\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 1s 659ms/step - loss: 1.3484\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 1.3481\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 1.3478\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 1s 630ms/step - loss: 1.3475\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 1s 612ms/step - loss: 1.3471\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 1s 665ms/step - loss: 1.3468\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 1.3465\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 1.3461\n",
      "Epoch 196/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 662ms/step - loss: 1.3458\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 1.3455\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 1s 631ms/step - loss: 1.3451\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.3448\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 1.3444\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 1s 658ms/step - loss: 1.3441\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 1s 670ms/step - loss: 1.3437\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 1s 672ms/step - loss: 1.3434\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 1s 634ms/step - loss: 1.3430\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.3427\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 1s 670ms/step - loss: 1.3423\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 1s 653ms/step - loss: 1.3420\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 1.3416\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 1s 669ms/step - loss: 1.3412\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 1.3409\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 1s 676ms/step - loss: 1.3405\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 1.3402\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 1s 631ms/step - loss: 1.3398\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 1.3394\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 1s 680ms/step - loss: 1.3391\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 1s 636ms/step - loss: 1.3387\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.3383\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 1s 656ms/step - loss: 1.3379\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 1.3376\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 1.3372\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 1s 686ms/step - loss: 1.3368\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 1s 627ms/step - loss: 1.3364\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 1s 639ms/step - loss: 1.3361\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 1.3357\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 1.3353\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 1.3349\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 1s 658ms/step - loss: 1.3346\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 1s 656ms/step - loss: 1.3342\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 1s 639ms/step - loss: 1.3338\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 1s 637ms/step - loss: 1.3334\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 1s 669ms/step - loss: 1.3330\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 1s 651ms/step - loss: 1.3327\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 1.3323\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 1.3319\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 1s 657ms/step - loss: 1.3315\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 1s 629ms/step - loss: 1.3311\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 1s 640ms/step - loss: 1.3308\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 1.3304\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 1s 643ms/step - loss: 1.3300\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 1s 650ms/step - loss: 1.3296\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 1s 641ms/step - loss: 1.3292\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 1s 690ms/step - loss: 1.3289\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 1.3285\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 1s 686ms/step - loss: 1.3281\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 1s 642ms/step - loss: 1.3277\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 1s 676ms/step - loss: 1.3273\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 1.3270\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 1.3266\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 1.3262\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 1.3258\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 1.3254\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 1s 669ms/step - loss: 1.3251\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 1s 679ms/step - loss: 1.3247\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 1s 669ms/step - loss: 1.3243\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 1.3239\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 1s 658ms/step - loss: 1.3236\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 1s 682ms/step - loss: 1.3232\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 1s 674ms/step - loss: 1.3228\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 1s 669ms/step - loss: 1.3224\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 1.3221\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 1s 646ms/step - loss: 1.3217\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 1.3213\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 1s 649ms/step - loss: 1.3210\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 1s 663ms/step - loss: 1.3206\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 1.3202\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 1s 698ms/step - loss: 1.3199\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 1s 681ms/step - loss: 1.3195\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 1s 658ms/step - loss: 1.3191\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 1s 638ms/step - loss: 1.3188\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 1s 650ms/step - loss: 1.3184\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 1s 662ms/step - loss: 1.3181\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 1.3177\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 1s 671ms/step - loss: 1.3174\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 1s 654ms/step - loss: 1.3170\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 1s 673ms/step - loss: 1.3167\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 1s 673ms/step - loss: 1.3163\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 1s 684ms/step - loss: 1.3160\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.3156\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 1s 653ms/step - loss: 1.3153\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.3149\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 1s 637ms/step - loss: 1.3146\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 1s 627ms/step - loss: 1.3142\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 1.3139\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.3136\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 1s 665ms/step - loss: 1.3132\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 1s 658ms/step - loss: 1.3129\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 1s 645ms/step - loss: 1.3126\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.3122\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 1.3119\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 1.3116\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 1s 640ms/step - loss: 1.3113\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 1s 657ms/step - loss: 1.3109\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 1s 641ms/step - loss: 1.3106\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 1.3103\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 1.3100\n",
      "Epoch 296/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 676ms/step - loss: 1.3097\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 1s 673ms/step - loss: 1.3094\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 1s 675ms/step - loss: 1.3091\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 1s 661ms/step - loss: 1.3087\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 1s 642ms/step - loss: 1.3084\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 1s 651ms/step - loss: 1.3081\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 1s 661ms/step - loss: 1.3078\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 1.3075\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 1s 680ms/step - loss: 1.3072\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 1.3070\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 1.3067\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 1s 650ms/step - loss: 1.3064\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 1s 661ms/step - loss: 1.3061\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 1s 650ms/step - loss: 1.3058\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 1s 658ms/step - loss: 1.3055\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 1s 657ms/step - loss: 1.3052\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 1s 645ms/step - loss: 1.3050\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 1s 658ms/step - loss: 1.3047\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 1s 649ms/step - loss: 1.3044\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 1s 640ms/step - loss: 1.3041\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 1s 644ms/step - loss: 1.3039\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 1s 653ms/step - loss: 1.3036\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 1.3033\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 1s 657ms/step - loss: 1.3031\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 1s 658ms/step - loss: 1.3028\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 1.3026\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 1s 659ms/step - loss: 1.3023\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 1.3021\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 1s 672ms/step - loss: 1.3018\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 1s 656ms/step - loss: 1.3016\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 1s 641ms/step - loss: 1.3013\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 1s 652ms/step - loss: 1.3011\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 1s 634ms/step - loss: 1.3008\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 1s 659ms/step - loss: 1.3006\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 1.3004\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 1s 656ms/step - loss: 1.3001\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 1s 640ms/step - loss: 1.2999\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 1s 650ms/step - loss: 1.2997\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 1.2995\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 1s 662ms/step - loss: 1.2992\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 1s 665ms/step - loss: 1.2990\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 1s 661ms/step - loss: 1.2988\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 1s 635ms/step - loss: 1.2986\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 1s 651ms/step - loss: 1.2984\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 1s 661ms/step - loss: 1.2982\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 1s 674ms/step - loss: 1.2979\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 1s 653ms/step - loss: 1.2977\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 1s 675ms/step - loss: 1.2975\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 1s 677ms/step - loss: 1.2973\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 1s 649ms/step - loss: 1.2971\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 1.2969\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 1s 642ms/step - loss: 1.2967\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 1s 671ms/step - loss: 1.2965\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 1.2964\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 1s 634ms/step - loss: 1.2962\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 1s 642ms/step - loss: 1.2960\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 1s 654ms/step - loss: 1.2958\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 1s 627ms/step - loss: 1.2956\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 1s 672ms/step - loss: 1.2954\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 1.2953\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 1s 656ms/step - loss: 1.2951\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 1s 654ms/step - loss: 1.2949\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 1.2947\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 1.2946\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 1s 642ms/step - loss: 1.2944\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 1s 677ms/step - loss: 1.2943\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 1s 674ms/step - loss: 1.2941\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 1s 692ms/step - loss: 1.2939\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 1s 658ms/step - loss: 1.2938\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 1s 677ms/step - loss: 1.2936\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.2935\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 1.2933\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 1s 659ms/step - loss: 1.2932\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 1s 671ms/step - loss: 1.2930\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 1s 689ms/step - loss: 1.2929\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 1s 649ms/step - loss: 1.2927\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 1s 665ms/step - loss: 1.2926\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 1s 674ms/step - loss: 1.2924\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 1s 651ms/step - loss: 1.2923\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 1s 669ms/step - loss: 1.2922\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 1s 646ms/step - loss: 1.2920\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 1s 652ms/step - loss: 1.2919\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 1s 656ms/step - loss: 1.2918\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 1s 673ms/step - loss: 1.2917\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 1s 661ms/step - loss: 1.2915\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 1s 659ms/step - loss: 1.2914\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 1s 640ms/step - loss: 1.2913\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 1s 641ms/step - loss: 1.2912\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 1s 644ms/step - loss: 1.2910\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 1s 642ms/step - loss: 1.2909\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 1.2908\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 1s 662ms/step - loss: 1.2907\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 1s 657ms/step - loss: 1.2906\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 1s 653ms/step - loss: 1.2905\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 1s 650ms/step - loss: 1.2904\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 1s 646ms/step - loss: 1.2903\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 1.2902\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.2901\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 1s 637ms/step - loss: 1.2900\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 1.2899\n",
      "Epoch 396/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 673ms/step - loss: 1.2898\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 1s 646ms/step - loss: 1.2897\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 1s 659ms/step - loss: 1.2896\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 1s 654ms/step - loss: 1.2895\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 1s 652ms/step - loss: 1.2894\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 1s 679ms/step - loss: 1.2893\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 1s 665ms/step - loss: 1.2892\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 1s 685ms/step - loss: 1.2891\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 1.2890\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 1s 676ms/step - loss: 1.2889\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 1.2889\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 1s 650ms/step - loss: 1.2888\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 1s 663ms/step - loss: 1.2887\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.2886\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 1s 659ms/step - loss: 1.2885\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 1s 684ms/step - loss: 1.2885\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 1s 641ms/step - loss: 1.2884\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 1s 632ms/step - loss: 1.2883\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 1s 632ms/step - loss: 1.2882\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 1s 673ms/step - loss: 1.2882\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 1s 661ms/step - loss: 1.2881\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 1s 636ms/step - loss: 1.2880\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 1s 637ms/step - loss: 1.2880\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 1s 623ms/step - loss: 1.2879\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 1s 619ms/step - loss: 1.2878\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 1s 629ms/step - loss: 1.2878\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 1.2877\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 1s 631ms/step - loss: 1.2876\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 1s 640ms/step - loss: 1.2876\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 1s 644ms/step - loss: 1.2875\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 1s 651ms/step - loss: 1.2875\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 1s 628ms/step - loss: 1.2874\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 1s 650ms/step - loss: 1.2873\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 1.2873\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 1s 634ms/step - loss: 1.2872\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 1s 618ms/step - loss: 1.2872\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 1s 625ms/step - loss: 1.2871\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 1.2871\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 1s 620ms/step - loss: 1.2870\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 1s 636ms/step - loss: 1.2870\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 1s 626ms/step - loss: 1.2869\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 1s 612ms/step - loss: 1.2869\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 1s 628ms/step - loss: 1.2868\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 1s 625ms/step - loss: 1.2868\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 1s 646ms/step - loss: 1.2867\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 1.2867\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 1s 659ms/step - loss: 1.2866\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 1s 634ms/step - loss: 1.2866\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 1.2866\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 1s 632ms/step - loss: 1.2865\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 1s 635ms/step - loss: 1.2865\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 1s 617ms/step - loss: 1.2864\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 1s 621ms/step - loss: 1.2864\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 1.2864\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 1s 625ms/step - loss: 1.2863\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 1s 636ms/step - loss: 1.2863\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 1s 642ms/step - loss: 1.2862\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 1s 623ms/step - loss: 1.2862\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 1s 628ms/step - loss: 1.2862\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 1s 644ms/step - loss: 1.2861\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 1s 622ms/step - loss: 1.2861\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 1s 625ms/step - loss: 1.2861\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 1s 627ms/step - loss: 1.2860\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 1s 624ms/step - loss: 1.2860\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 1s 640ms/step - loss: 1.2860\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 1s 633ms/step - loss: 1.2859\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 1s 609ms/step - loss: 1.2859\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 1s 622ms/step - loss: 1.2859\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 1s 629ms/step - loss: 1.2859\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2858Restoring model weights from the end of the best epoch: 445.\n",
      "1/1 [==============================] - 1s 633ms/step - loss: 1.2858\n",
      "Epoch 465: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Import custom layers\n",
    "import models.layers_update_mobley as layers\n",
    "import importlib\n",
    "importlib.reload(layers)\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "class PGGCNModel(tf.keras.Model):\n",
    "    def __init__(self, num_atom_features=36, r_out_channel=20, c_out_channel=128):\n",
    "        super().__init__()\n",
    "        self.ruleGraphConvLayer = layers.RuleGraphConvLayer(r_out_channel, num_atom_features, 0)\n",
    "        self.ruleGraphConvLayer.combination_rules = []\n",
    "        self.conv = layers.ConvLayer(c_out_channel, r_out_channel)\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation='relu', name='dense1')\n",
    "        self.dense5 = tf.keras.layers.Dense(16, activation='relu', name='dense2')\n",
    "        self.dense6 = tf.keras.layers.Dense(1, name='dense6')\n",
    "        self.dense7 = tf.keras.layers.Dense(1, name='dense7',\n",
    "                                             kernel_initializer=tf.keras.initializers.Constant([-.3, 1, -1, -1]),\n",
    "                                             bias_initializer=tf.keras.initializers.Zeros())\n",
    "        self.all_layer_1_weights = []\n",
    "\n",
    "    def addRule(self, rule, start_index, end_index=None):\n",
    "        self.ruleGraphConvLayer.addRule(rule, start_index, end_index)\n",
    "\n",
    "    def set_input_shapes(self, i_s):\n",
    "        self.i_s = i_s\n",
    "\n",
    "    def call(self, inputs):\n",
    "        physics_info = inputs[:, 0, 38:]\n",
    "        x_a = []\n",
    "        for i in range(len(self.i_s)):\n",
    "            x_a.append(inputs[i][:self.i_s[i], :38])\n",
    "        x = self.ruleGraphConvLayer(x_a)\n",
    "        self.all_layer_1_weights.append(self.ruleGraphConvLayer.w_s)\n",
    "        x = self.conv(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense5(x)\n",
    "        model_var = self.dense6(x)\n",
    "        merged = tf.concat([model_var, physics_info], axis=1)\n",
    "        out = self.dense7(merged)\n",
    "        return out\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred[0] - y_true))) + K.abs(1 / K.mean(.2 + y_pred[1]))\n",
    "\n",
    "\n",
    "def pure_rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "m = PGGCNModel()\n",
    "m.addRule(\"sum\", 0, 32)\n",
    "m.addRule(\"multiply\", 32, 33)\n",
    "m.addRule(\"distance\", 33, 36)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "m.compile(loss=pure_rmse, optimizer=opt)\n",
    "input_shapes = []\n",
    "for i in range(len(X_train)):\n",
    "    input_shapes.append(np.array(X_train[i]).shape[0])\n",
    "m.set_input_shapes(input_shapes)\n",
    "for i in range(len(X_train)):\n",
    "    if X_train[i].shape[0] < 2000:\n",
    "        new_list = np.zeros([2000 - X_train[i].shape[0], 41])\n",
    "        X_train[i] = np.concatenate([X_train[i], new_list], 0)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='loss',           \n",
    "    patience=20,              \n",
    "    restore_best_weights=True, \n",
    "    min_delta=0.001,          \n",
    "    verbose=1                 \n",
    ")\n",
    "hist = m.fit(X_train, y_train, epochs = 600, batch_size=len(X_train), callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c9656f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d30db7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hist is the History object returned by .fit()\n",
    "plt.plot(hist.history['loss'], label='train loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6433e76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1085\n",
      "The mean absolute difference between y_tru & y_pred is : 5.770160050690386\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "input_shapes = []\n",
    "for i in range(len(X_test)):\n",
    "    input_shapes.append(np.array(X_test[i]).shape[0])\n",
    "m.set_input_shapes(input_shapes)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    if X_test[i].shape[0] < 2000:\n",
    "        new_list = np.zeros([2000 - X_test[i].shape[0], 41])\n",
    "        X_test[i] = np.concatenate([X_test[i], new_list], 0)\n",
    "X_test = np.array(X_test)\n",
    "x_c = copy.deepcopy(X_test)\n",
    "y_test = np.array(y_test)\n",
    "y_pred_test = m.predict(X_test)\n",
    "y_pred_test = np.array(y_pred_test)\n",
    "y_difference = np.mean(np.abs(np.abs(y_test) - np.abs(y_pred_test)))\n",
    "eval = m.evaluate(X_test, y_test)\n",
    "print(\"The mean absolute difference between y_tru & y_pred is : {}\" .format(str(y_difference)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03866fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
