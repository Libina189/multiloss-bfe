{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3518e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 19:05:41.516125: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-30 19:05:41.583399: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (/home/lthoma21/.local/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/lthoma21/.local/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import conda_installer\n",
    "from rdkit import Chem\n",
    "from deepchem.feat.graph_features import atom_features as get_atom_features\n",
    "import rdkit\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd1d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDBs = pickle.load(open('Datasets/PDBs_RDKit.pkl', 'rb'))\n",
    "df = pd.read_csv('Datasets/T_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b983c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83bd5e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(PDBs), df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e9e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "for pdb in list(PDBs.keys()):\n",
    "    info.append(df[df['Id'] == pdb][['TS_comp', 'TS_host', 'TS_ligand']].to_numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e0adae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4dda9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.dcFeaturizer import atom_features as get_atom_features\n",
    "def featurize(molecule, info):\n",
    "    \n",
    "    atom_features = []\n",
    "    for atom in molecule.GetAtoms():\n",
    "        new_feature = get_atom_features(atom).tolist()\n",
    "        position = molecule.GetConformer().GetAtomPosition(atom.GetIdx())\n",
    "        new_feature += [atom.GetMass(), atom.GetAtomicNum(),atom.GetFormalCharge()]\n",
    "        new_feature += [position.x, position.y, position.z]\n",
    "        for neighbor in atom.GetNeighbors()[:2]:\n",
    "            neighbor_idx = neighbor.GetIdx()\n",
    "            new_feature += [neighbor_idx]\n",
    "        for i in range(2 - len(atom.GetNeighbors())):\n",
    "            new_feature += [-1]\n",
    "\n",
    "        atom_features.append(np.concatenate([new_feature, info], 0))\n",
    "    return np.array(atom_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa4320fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i, pdb in enumerate(list(PDBs.keys())):\n",
    "    X.append(featurize(PDBs[pdb], info[i]))\n",
    "    y.append(df[df['Id'] == pdb]['exp'].to_numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f57276a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 41), -0.9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape, y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1978b2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a7c4d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# Randomly shuffles the data before splitting, ensuring that the training and testing sets are representative of the overall dataset.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f4d8810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 21, 82, 21)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f709fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, x in enumerate(X_test):\n",
    "#     print(f\"Sample {i}: shape = {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9814dd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 19:06:35.995321: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-04-30 19:06:35.995356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (curie.cluster): /proc/driver/nvidia/version does not exist\n",
      "2025-04-30 19:06:35.996134: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/calstatela/mambaforge/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "1/1 [==============================] - 81s 81s/step - loss: 13.0894\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.9778\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 1s 859ms/step - loss: 8.9258\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.9871\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.2876\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.1243\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9238\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.5183\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.2145\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.6633\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.8045\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.6759\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.3443\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.8915\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.4168\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.0379\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8631\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9196\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.1202\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.3361\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.4760\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.5026\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.4202\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.2610\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.0757\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9220\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8456\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8584\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9319\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.0178\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.0738\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.0792\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.0356\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9624\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8881\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8392\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8287\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8513\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8876\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9162\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9232\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.9066\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8752\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8430\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8227\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8199\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8312\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8469\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8572\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8567\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8460\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8310\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8185\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8134\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8159\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8226\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8284\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8296\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8257\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8188\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8126\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8096\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8103\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8132\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8157\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8161\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8140\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8107\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8078\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8065\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8069\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8081\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8089\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8086\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8072\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8054\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8041\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8036\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8039\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8042\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8042\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8036\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8026\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8016\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8010\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8008\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8008\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8006\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8002\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7995\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7988\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7983\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7979\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7977\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7974\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7970\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7965\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7959\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7954\n",
      "Epoch 100/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 3.7950\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7946\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7943\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7938\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7933\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7928\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7923\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7919\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7915\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7911\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7906\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7901\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7896\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7891\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7887\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7882\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7877\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7873\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7868\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7862\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7858\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7853\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7848\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7843\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7838\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7832\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7827\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7822\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7817\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7812\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7806\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7801\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7795\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7790\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7785\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7779\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7774\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7768\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7762\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7757\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7751\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7745\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7739\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7733\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7727\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7721\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7715\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7709\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7703\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7697\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7691\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7684\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7678\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7672\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7665\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7659\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7652\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7646\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7639\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7632\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7626\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7619\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7612\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7605\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7598\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7591\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7584\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7577\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7569\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7562\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7555\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7547\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7540\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7532\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7524\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7517\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7509\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7501\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7493\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7485\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7477\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7469\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7461\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7452\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7444\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7435\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7427\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7418\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7410\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7401\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7392\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7383\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7374\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7365\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7356\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7347\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7337\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7328\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7319\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7309\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7299\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7290\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7280\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7270\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7260\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7250\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7240\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7229\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7219\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7208\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7198\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7187\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7177\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7166\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7155\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7144\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7133\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7121\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7110\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7099\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7087\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7076\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7064\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7052\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7040\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7028\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7016\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.7004\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6992\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6979\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6967\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6954\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6942\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6929\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6916\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6903\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6890\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6876\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6863\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6850\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6836\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6823\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6809\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6795\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6781\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6767\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6753\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6739\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6724\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6710\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6695\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6681\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6666\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6651\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6636\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6621\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6606\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6590\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6575\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6559\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6544\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6528\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6512\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6496\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6480\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6464\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6448\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6432\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6415\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6399\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6382\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6365\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6348\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6331\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6314\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6297\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6280\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6263\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6245\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6228\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6210\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6192\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6174\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6156\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6138\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6120\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6102\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6084\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6065\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6047\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6028\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6010\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5991\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5972\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5953\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5934\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5915\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5895\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5876\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5857\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5837\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5818\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5798\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5778\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5758\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5738\n",
      "Epoch 306/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 3.5718\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5698\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5678\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5658\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5638\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5617\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5597\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5576\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5555\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5535\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5514\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5493\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5472\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5451\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5430\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5409\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5388\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5366\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5345\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5324\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5302\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5281\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5259\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5237\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5216\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5194\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5172\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5150\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5128\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5106\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5084\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5062\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5040\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5017\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4995\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4973\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4950\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4928\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4905\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4883\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4860\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4837\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4815\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4792\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4769\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4746\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4723\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4700\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4678\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4655\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4632\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4608\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4585\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4562\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4539\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4516\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4493\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4469\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4446\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4423\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4400\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4376\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4353\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4329\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4306\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4283\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4259\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4236\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4212\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4189\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4165\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4142\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4118\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4094\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4071\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4047\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4024\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.4000\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3977\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3953\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3929\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3906\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3882\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3858\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3835\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3811\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3788\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3764\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3740\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3717\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3693\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3670\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3646\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3622\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3599\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3575\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3552\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3528\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3505\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3481\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3458\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3434\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3411\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3387\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3364\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3341\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3317\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3294\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3271\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3247\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3224\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3201\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3178\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3155\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3132\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3108\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3085\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3062\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3039\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.3016\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2994\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2971\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2948\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2925\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2902\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2880\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2857\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2834\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2812\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2789\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2767\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2744\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2722\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2699\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2677\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2655\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2633\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2611\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2589\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2567\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2545\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2523\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2501\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2479\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2457\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2436\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2414\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2392\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2371\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2349\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2328\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2307\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2285\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2264\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2243\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2222\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2201\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2180\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2159\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2139\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2118\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2097\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2077\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2056\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2036\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2016\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1995\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1975\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1955\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1935\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1915\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1895\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1875\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1856\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1836\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1817\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1797\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1778\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1758\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1739\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1720\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1701\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1682\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1663\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1644\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1626\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1607\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1588\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1570\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1552\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1533\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1515\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1497\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1479\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1461\n"
     ]
    }
   ],
   "source": [
    "# Import custom layers\n",
    "import importlib\n",
    "import models.layers_update_mobley as layers\n",
    "import keras.backend as K\n",
    "import copy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "class PGGCNModel(tf.keras.Model):\n",
    "    def __init__(self, num_atom_features=36, r_out_channel=20, c_out_channel=128):\n",
    "        super().__init__()\n",
    "        self.ruleGraphConvLayer = layers.RuleGraphConvLayer(r_out_channel, num_atom_features, 0)\n",
    "        self.ruleGraphConvLayer.combination_rules = []\n",
    "        self.conv = layers.ConvLayer(c_out_channel, r_out_channel)\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation='relu', name='dense1')\n",
    "        self.dense5 = tf.keras.layers.Dense(16, activation='relu', name='dense2')\n",
    "        self.dense6 = tf.keras.layers.Dense(1, name='dense6')\n",
    "        self.dense7 = tf.keras.layers.Dense(1, name='dense7',\n",
    "                                            kernel_initializer=tf.keras.initializers.Constant([-.3, -1, 1, 1]),\n",
    "                                            bias_initializer=tf.keras.initializers.Zeros())\n",
    "\n",
    "    def addRule(self, rule, start_index, end_index=None):\n",
    "        self.ruleGraphConvLayer.addRule(rule, start_index, end_index)\n",
    "\n",
    "    def set_input_shapes(self, i_s):\n",
    "        self.i_s = i_s\n",
    "\n",
    "    def call(self, inputs):\n",
    "        physics_info = inputs[:, 0, 38:]\n",
    "        x_a = []\n",
    "        for i in range(len(self.i_s)):\n",
    "            x_a.append(inputs[i][:self.i_s[i], :38])\n",
    "        x = self.ruleGraphConvLayer(x_a)\n",
    "        x = self.conv(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense5(x)\n",
    "        model_var = self.dense6(x)\n",
    "        merged = tf.concat([model_var, physics_info], axis=1)\n",
    "        out = self.dense7(merged)\n",
    "        return out\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred[0] - y_true))) + K.abs(1 / K.mean(.2 + y_pred[1]))\n",
    "\n",
    "\n",
    "def pure_rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "m = PGGCNModel()\n",
    "m.addRule(\"sum\", 0, 32)\n",
    "m.addRule(\"multiply\", 32, 33)\n",
    "m.addRule(\"distance\", 33, 36)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "m.compile(loss=pure_rmse, optimizer=opt)\n",
    "\n",
    "input_shapes = []\n",
    "for i in range(len(X_train)):\n",
    "    input_shapes.append(np.array(X_train[i]).shape[0])\n",
    "m.set_input_shapes(input_shapes)\n",
    "for i in range(len(X_train)):\n",
    "    if X_train[i].shape[0] < 2000:\n",
    "        new_list = np.zeros([2000 - X_train[i].shape[0], 41])\n",
    "        X_train[i] = np.concatenate([X_train[i], new_list], 0)\n",
    "X_train = np.array(X_train)\n",
    "x_c = copy.deepcopy(X_train)\n",
    "y_train = np.array(y_train)\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='loss',           \n",
    "    patience=10,              \n",
    "    restore_best_weights=True, \n",
    "    min_delta=0.001,          \n",
    "    verbose=1                 \n",
    ")\n",
    "hist = m.fit(X_train, y_train, epochs = 500, batch_size=len(X_train), callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c071b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f47f1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hist is the History object returned by .fit()\n",
    "plt.plot(hist.history['loss'], label='train loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99694058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.4778\n",
      "The mean absolute difference between y_tru & y_pred is : 1.3101387346899156\n",
      "[[-0.0121755 ]\n",
      " [-2.4804068 ]\n",
      " [-1.7435293 ]\n",
      " [ 0.03807169]\n",
      " [ 0.8356028 ]\n",
      " [ 0.22166544]\n",
      " [-0.6844167 ]\n",
      " [ 2.5064402 ]\n",
      " [ 2.883607  ]\n",
      " [-0.4394216 ]\n",
      " [ 1.3659525 ]\n",
      " [-0.34461874]\n",
      " [ 1.8701029 ]\n",
      " [-0.5214986 ]\n",
      " [ 1.1382761 ]\n",
      " [ 1.4578104 ]\n",
      " [ 1.9614878 ]\n",
      " [-3.5596914 ]\n",
      " [ 1.5446177 ]\n",
      " [-2.9054098 ]\n",
      " [-2.9132223 ]]\n",
      "[ 0.2    0.66   0.91  -0.87  -0.1   -0.8   -4.79   0.3   -1.2    0.68\n",
      " -0.3   -0.6    0.9   -3.59  -1.6    0.61   3.7    0.774 -2.049 -0.552\n",
      " -0.1  ]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "input_shapes = []\n",
    "for i in range(len(X_test)):\n",
    "    input_shapes.append(np.array(X_test[i]).shape[0])\n",
    "m.set_input_shapes(input_shapes)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    if X_test[i].shape[0] < 2000:\n",
    "        new_list = np.zeros([2000 - X_test[i].shape[0], 41])\n",
    "        X_test[i] = np.concatenate([X_test[i], new_list], 0)\n",
    "X_test = np.array(X_test)\n",
    "x_c = copy.deepcopy(X_test)\n",
    "y_test = np.array(y_test)\n",
    "y_pred_test = m.predict(X_test) \n",
    "y_pred_test = np.array(y_pred_test)\n",
    "y_difference = np.mean(np.abs(np.abs(y_test) - np.abs(y_pred_test)))\n",
    "eval = m.evaluate(X_test, y_test)\n",
    "print(\"The mean absolute difference between y_tru & y_pred is : {}\" .format(str(y_difference)))\n",
    "print(y_pred_test)\n",
    "print(y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834c1860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
